<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>YiFei Zhang&#39;s Blog</title>
    <meta name="description" content="在这里了解我的一切，对编程的热爱永不停歇。">
    <meta name="generator" content="VuePress 1.3.0">
    <link rel="stylesheet" href="https://cdn.bootcss.com/prism/9000.0.1/themes/prism.min.css">
  <link rel="stylesheet" href="https://cdn.bootcss.com/KaTeX/0.5.1/katex.min.css">
  <link rel="icon" type="image/png" href="/favicons/icon.png">
    
    <link rel="preload" href="/assets/css/0.styles.a0fda9c7.css" as="style"><link rel="preload" href="/assets/js/app.98c20e97.js" as="script"><link rel="preload" href="/assets/js/11.57fd9d89.js" as="script"><link rel="preload" href="/assets/js/7.2b98330f.js" as="script"><link rel="preload" href="/assets/js/2.b7c0b55c.js" as="script"><link rel="preload" href="/assets/js/6.72135ada.js" as="script"><link rel="prefetch" href="/assets/js/10.eb39f85e.js"><link rel="prefetch" href="/assets/js/12.7564357d.js"><link rel="prefetch" href="/assets/js/13.9af6e9b2.js"><link rel="prefetch" href="/assets/js/14.db6268ad.js"><link rel="prefetch" href="/assets/js/15.da0dc55a.js"><link rel="prefetch" href="/assets/js/16.2d5ffbec.js"><link rel="prefetch" href="/assets/js/17.639b981e.js"><link rel="prefetch" href="/assets/js/18.728bff98.js"><link rel="prefetch" href="/assets/js/19.2739cfd3.js"><link rel="prefetch" href="/assets/js/20.6c67605d.js"><link rel="prefetch" href="/assets/js/21.7e8f9d8d.js"><link rel="prefetch" href="/assets/js/22.bd6a575c.js"><link rel="prefetch" href="/assets/js/23.9caa854b.js"><link rel="prefetch" href="/assets/js/24.19f984ff.js"><link rel="prefetch" href="/assets/js/25.13074256.js"><link rel="prefetch" href="/assets/js/26.cb0deb68.js"><link rel="prefetch" href="/assets/js/27.0efdadd4.js"><link rel="prefetch" href="/assets/js/28.7dacb917.js"><link rel="prefetch" href="/assets/js/29.fadb250a.js"><link rel="prefetch" href="/assets/js/3.3d15d864.js"><link rel="prefetch" href="/assets/js/30.9c9aa65d.js"><link rel="prefetch" href="/assets/js/31.c0835d50.js"><link rel="prefetch" href="/assets/js/32.a9088180.js"><link rel="prefetch" href="/assets/js/33.42bf1b2e.js"><link rel="prefetch" href="/assets/js/34.8bbdd14d.js"><link rel="prefetch" href="/assets/js/4.458a9fae.js"><link rel="prefetch" href="/assets/js/5.a1f54807.js"><link rel="prefetch" href="/assets/js/8.601360c2.js"><link rel="prefetch" href="/assets/js/9.023ae887.js">
    <link rel="stylesheet" href="/assets/css/0.styles.a0fda9c7.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="cv" data-v-4e646bac><div class="header-wrap" data-v-a542e14c data-v-4e646bac><div class="header page" data-v-a542e14c><div class="left" data-v-a542e14c><div class="motto" data-v-a542e14c></div> <div class="nav" data-v-a542e14c></div></div> <div class="right" data-v-a542e14c><div class="search-box" data-v-a542e14c><input aria-label="Search" placeholder="" autocomplete="off" spellcheck="false" value=""> <!----></div></div></div></div> <div class="page cv-content" data-v-4e646bac><div class="cv-header cv-two-column cv-section" data-v-4e646bac><div class="cv-info" data-v-4e646bac><div class="cv-author" data-v-4e646bac>张义飞</div> <div class="cv-contact" data-v-4e646bac>电话：+86 13032883129</div> <div class="cv-contact" data-v-4e646bac>邮箱：flynn.zhang@foxmail.com</div> <div class="cv-contact" data-v-4e646bac>主页：blog.simplenaive.cn</div> <div class="cv-contact" data-v-4e646bac>Github：github.com/Yidadaa</div></div> <div class="cv-sub-info cv-right-column" data-v-4e646bac><div class="cv-research-interest" data-v-4e646bac>三维视觉</div> <div class="cv-dob" data-v-4e646bac>DOB: 1997/01/20</div> <div class="cv-city" data-v-4e646bac>Chengdu, China</div></div></div> <div class="cv-two-column" data-v-4e646bac><div class="cv-section education cv-left-column" data-v-4e646bac><div class="cv-section-title" data-v-4e646bac>教育背景</div> <div class="cv-two-column" data-v-4e646bac><div class="cv-left-column cv-school" data-v-4e646bac>硕士，电子科技大学</div> <div class="cv-right-column cv-time-range" data-v-4e646bac>2018.09 - 2021.06</div></div> <div class="cv-major" data-v-4e646bac>计算机科学与工程学院，计算机科学专业</div> <div class="cv-two-column" data-v-4e646bac><div class="cv-left-column cv-school" data-v-4e646bac>硕士，电子科技大学</div> <div class="cv-right-column cv-time-range" data-v-4e646bac>2018.09 - 2021.06</div></div> <div class="cv-major" data-v-4e646bac>计算机科学与工程学院，计算机科学专业</div></div> <div class="cv-section honor cv-right-column" data-v-4e646bac><div class="cv-section-title" data-v-4e646bac>荣誉 &amp; 奖项</div> <div class="cv-two-column" data-v-4e646bac><div class="cv-left-column cv-honor" data-v-4e646bac>研究生二等学业奖学金</div> <div class="cv-right-column cv-time-range" data-v-4e646bac>2019.10</div></div> <div class="cv-two-column" data-v-4e646bac><div class="cv-left-column cv-honor" data-v-4e646bac>OPPO AI 挑战赛人像分割任务 决赛优秀奖</div> <div class="cv-right-column cv-time-range" data-v-4e646bac>2019.04</div></div> <div class="cv-two-column" data-v-4e646bac><div class="cv-left-column cv-honor" data-v-4e646bac>研究生一等学业奖学金</div> <div class="cv-right-column cv-time-range" data-v-4e646bac>2018.10</div></div></div></div> <div class="cv-section experience" data-v-4e646bac><div class="cv-section-title" data-v-4e646bac>实习 &amp; 研究经历</div> <ul class="cv-experience-list" data-v-4e646bac><li class="cv-experience-item" data-v-4e646bac><div class="cv-two-column cv-experience-header" data-v-4e646bac><div class="cv-left-column cv-experience-title" data-v-4e646bac>三维视觉中的深度估计算法</div> <div class="cv-right-column cv-time-range" data-v-4e646bac>from 2019.04</div></div> <ul class="cv-experience-sub-item" data-v-4e646bac><li class="cv-li" data-v-4e646bac>研究⽅向是基于单目视频序列的无监督深度估计算法，基于深度神经⽹络的无监督训练流程可以很快地被迁移到全新环境中，并且训练时只需要单⽬摄像头视频数据，⾮常有应⽤前景。</li> <li class="cv-li" data-v-4e646bac>当前的想法：现有的无监督训练流程有着收敛过慢的缺点，尝试从以下⼏个⽅⾯改进：Co-attention 模块已被证明可以很好地挖掘相邻视频帧中具有较⾼相似度的特征区域，所以尝试在特征图层⾯对相邻帧的特征进⾏ attention，使得特征相似度较⾼的区域产⽣更相近的深度图；其次⽬前无监督流程中的 PoseNet 的⾃监督信号完全来⾃于与残差图估计⽹络联合建⽴的重建损失，所以提出尝试使⽤ SLAM 系统后端中的滤波⽅法对 PoseNet 的输出做约束，以期提升 PoseNet 的性能。</li></ul></li> <li class="cv-experience-item" data-v-4e646bac><div class="cv-two-column cv-experience-header" data-v-4e646bac><div class="cv-left-column cv-experience-title" data-v-4e646bac>基于 Unity 虚拟环境和强化学习的机械臂控制算法 @ 毕业设计</div> <div class="cv-right-column cv-time-range" data-v-4e646bac>2017.10 - 2018.06</div></div> <ul class="cv-experience-sub-item" data-v-4e646bac><li class="cv-li" data-v-4e646bac>强化学习算法在机械智能控制中愈发重要，然而强化学习算法往往需要在训练阶段通过大量地 exploration 与环境交互取得数据来优化策略，这种训练策略将会在真实世界带来极高的时间和物料成本，本文提出了一种架构，可以使得现有强化学习模型可以实时与 Unity 中的虚拟环境交互并获取训练数据，为迁移至真实环境提供预训练基础。</li> <li class="cv-li" data-v-4e646bac>本文主要工作如下：1) 构建了一个沟通 Unity 运行时和 Python 运行时的中间层，使得基于 Tensorflow 和 Pytorch 的构建的
				深度学习模型可以与 Unity 虚拟环境交互；2) 实现了强化学习中经典的 PPO 算法，并分别在二维和三维环境中设计对应的任务来验证算法和中间层的可用性；
				3) 基于本文提出的架构，探讨了多线程以及离屏或低分辨率渲染等手段对训练任务的加速能力。</li></ul></li></ul></div> <div class="cv-section coding" data-v-4e646bac><div class="cv-section-title" data-v-4e646bac>开源项目 &amp; 编程能力</div> <div class="cv-coding-item" data-v-4e646bac><div class="cv-two-column" data-v-4e646bac><div class="cv-left-column cv-coding-title" data-v-4e646bac><a href="https://github.com/Yidadaa/Pytorch-Video-Classification" class="cv-coding-link" data-v-4e646bac>github.com/Yidadaa/Pytorch-Video-Classification</a> <span class="cv-coding-language" data-v-4e646bac>(Python / Pytorch) ~ 500 lines</span></div> <div class="cv-right-column cv-time-range" data-v-4e646bac>2019.04</div></div> <div class="cv-coding-desc" data-v-4e646bac>基于 CNN-RNN 架构的视频动作分类⽹络，在 UCF101 上达到 80% 的准确率。</div></div> <div class="cv-coding-item" data-v-4e646bac><div class="cv-two-column" data-v-4e646bac><div class="cv-left-column cv-coding-title" data-v-4e646bac><a href="https://github.com/Yidadaa/Pytorch-Video-Classification" class="cv-coding-link" data-v-4e646bac>github.com/Yidadaa/Pytorch-Video-Classification</a> <span class="cv-coding-language" data-v-4e646bac>(Python / Pytorch) ~ 500 lines</span></div> <div class="cv-right-column cv-time-range" data-v-4e646bac>2019.04</div></div> <div class="cv-coding-desc" data-v-4e646bac>基于 CNN-RNN 架构的视频动作分类⽹络，在 UCF101 上达到 80% 的准确率。</div></div> <div class="cv-coding-item" data-v-4e646bac><div class="cv-two-column" data-v-4e646bac><div class="cv-left-column cv-coding-title" data-v-4e646bac><a href="https://github.com/Yidadaa/Pytorch-Video-Classification" class="cv-coding-link" data-v-4e646bac>github.com/Yidadaa/Pytorch-Video-Classification</a> <span class="cv-coding-language" data-v-4e646bac>(Python / Pytorch) ~ 500 lines</span></div> <div class="cv-right-column cv-time-range" data-v-4e646bac>2019.04</div></div> <div class="cv-coding-desc" data-v-4e646bac>基于 CNN-RNN 架构的视频动作分类⽹络，在 UCF101 上达到 80% 的准确率。</div></div></div></div> <div class="footer" data-v-87c5cddc data-v-4e646bac><div class="footer-content page" data-v-87c5cddc><div class="footer-title power" data-v-87c5cddc>本博客驱动自</div> <a href="https://github.com/Yidadaa/Issue-Blog-With-Github-Action" class="logo" data-v-87c5cddc>ISSUE BLOG</a> <div class="footer-title" data-v-87c5cddc>友情链接</div> <div class="links" data-v-87c5cddc><a href="sssssss" class="link" data-v-87c5cddc>xxxxx</a><a href="sssssss" class="link" data-v-87c5cddc>xxxxx</a><a href="sssssss" class="link" data-v-87c5cddc>xxxxx</a><a href="sssssss" class="link" data-v-87c5cddc>xxxxx</a></div></div></div></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.98c20e97.js" defer></script><script src="/assets/js/11.57fd9d89.js" defer></script><script src="/assets/js/7.2b98330f.js" defer></script><script src="/assets/js/2.b7c0b55c.js" defer></script><script src="/assets/js/6.72135ada.js" defer></script>
  </body>
</html>
